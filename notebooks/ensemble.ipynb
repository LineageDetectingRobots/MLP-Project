{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from framework import MODEL_PATH, RESULTS_PATH\n",
    "from framework.utils.experiment_utils import get_model, check_network_name, get_datasets, get_data_loader\n",
    "from framework.config.config_reader import ConfigReader\n",
    "\n",
    "\n",
    "def load_model(model_path, model_settings):\n",
    "    model = get_model(model_settings)\n",
    "    model_path = os.path.join(MODEL_PATH, model_path + '.pt')\n",
    "    if not os.path.exists(model_path):\n",
    "        raise RuntimeError('Unkown {} model path'.format(model_path))\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "def get_scores(model, dataset):\n",
    "    model.eval()\n",
    "    cuda = model._is_on_cuda()\n",
    "    dataloader = get_data_loader(dataset, 1, cuda)\n",
    "\n",
    "    scores_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            x = dataset[i][0].to(model._device(), dtype=torch.float).view(1, -1)\n",
    "            y_hat = model(x)\n",
    "            scores_list.append(y_hat)\n",
    "    return torch.stack(scores_list)\n",
    "\n",
    "def eval(pred_targets, dataset):\n",
    "    targets = []\n",
    "    targets_fmd = []\n",
    "    targets_fms = []\n",
    "    pred_targets_fmd = []\n",
    "    pred_targets_fms = []\n",
    "    for i in range(len(dataset)):\n",
    "        y = dataset[i][1]\n",
    "        targets.append(y)\n",
    "        \n",
    "        type_relation = dataset.dataset.iloc[i].type\n",
    "        tripair = dataset._get_tripair(i)\n",
    "        \n",
    "        if type_relation == 'fmd':\n",
    "            targets_fmd.append(y)\n",
    "            pred_targets_fmd.append(pred_targets[i])\n",
    "        elif type_relation == 'fms':\n",
    "            targets_fms.append(y)\n",
    "            pred_targets_fms.append(pred_targets[i])\n",
    "        else:\n",
    "            raise RuntimeError('Unkown relationship type = {}'.format(type_relation))\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(np.array(targets), pred_targets)\n",
    "    fmd_auc = roc_auc_score(targets_fmd, pred_targets_fmd)\n",
    "    fms_auc = roc_auc_score(targets_fms, pred_targets_fms)\n",
    "    return {'auc': auc,\n",
    "            'fmd_auc': fmd_auc,\n",
    "            'fms_auc': fms_auc}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE This should be a list of profiles you want to use. The models are expected to be saved\n",
    "model_profiles = ['DROP_TWO_DEC_vgg2', 'DROP_TWO_DEC_arc', 'DROP_TWO_DEC_vgg', 'DROP_TWO_DEC_sphere']\n",
    "model_profiles = ['DROP_TWO_DEC_arc', 'DROP_TWO_DEC_vgg2', 'DROP_TWO_DEC_sphere']\n",
    "ensemble_name = 'majority_vote'\n",
    "if model_profiles == []:\n",
    "    raise RuntimeError('Please fill in model profiles')\n",
    "\n",
    "models = []\n",
    "test_datasets = []\n",
    "for profile in model_profiles:\n",
    "    config_data = ConfigReader(profile).config_data\n",
    "\n",
    "    # Get test datasets for each model\n",
    "    network_name = config_data['data_settings']['network_name']\n",
    "    check_network_name(network_name)\n",
    "    train_dataset, validation_dataset, test_dataset, vec_length = get_datasets(network_name)\n",
    "#     test_datasets.append(test_dataset)\n",
    "    test_datasets.append(validation_dataset)\n",
    "\n",
    "    model_settings = config_data['model_settings']\n",
    "    model_settings['input_size'] = vec_length\n",
    "    experiment_name = config_data['experiment_name']\n",
    "    models.append(load_model(experiment_name, model_settings))\n",
    "\n",
    "# A list of lists of scores\n",
    "# NOTE: scores[0] -> scores of model 0 in models list \n",
    "scores = torch.stack([get_scores(model, test_dataset) for model, test_dataset in zip(models, test_datasets)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_score(scores):\n",
    "    # 4, test_size, 1, 2\n",
    "    result = torch.sum(scores, dim=0)\n",
    "    # test_size, 1, 2\n",
    "    result = torch.max(result, dim=2)[1]\n",
    "    return result.cpu().numpy()\n",
    "\n",
    "def majority_vote(scores):\n",
    "    n_models = list(scores.size())[0]\n",
    "    results = scores.cpu().numpy()\n",
    "    result = np.argmax(results, axis=3)\n",
    "    result = np.sum(result)\n",
    "    result = result/ n_models\n",
    "    # Note: Not >= because if it is 50/50 more likely to be not related     \n",
    "    result = (result > 0.5).astype(int)\n",
    "    return result\n",
    "\n",
    "def threshold_majority_vote(scores):\n",
    "    scores = scores.cpu().numpy()\n",
    "    threshold = 0.85\n",
    "    pred_targets = []\n",
    "    n_models = scores.shape[0]\n",
    "    # Iterate over batch size\n",
    "    for i in range(scores.shape[1]):\n",
    "        # NOTE: Assuming order best model to worst model\n",
    "        # Iterate over models\n",
    "        votes = []\n",
    "        for j in range(scores.shape[0]):\n",
    "            pred_score = scores[j, i]\n",
    "            if pred_score[0][1] > threshold:\n",
    "                votes.append(1)\n",
    "            if pred_score[0][0] > threshold:\n",
    "                votes.append(0)\n",
    "        \n",
    "        if votes == []:\n",
    "            # TODO: maj vote of scores\n",
    "            model_scores = np.array([scores[j, i] for j in range(scores.shape[0])])\n",
    "            result = np.argmax(model_scores, axis = 2)\n",
    "            result = np.sum(result)\n",
    "            result = result/n_models\n",
    "            \n",
    "        else:\n",
    "            result = sum(votes)/len(votes)\n",
    "\n",
    "        if result < 0.5:\n",
    "            result = 0\n",
    "        elif result == 0.5:\n",
    "            result = np.argmax(scores[0, i][0])\n",
    "        else:\n",
    "            result = 1\n",
    "        pred_targets.append(result)\n",
    "    return pred_targets\n",
    "                \n",
    "            \n",
    "def maj_vote_cascade(scores):\n",
    "    result = np.argmax(scores, axis=2)\n",
    "    result = sum(result)/len(result)\n",
    "    result = int(round(result[0]))\n",
    "    return result\n",
    "\n",
    "def cascade_classifier(scores):\n",
    "    pred_targets = []\n",
    "    scores = scores.cpu().numpy()\n",
    "    # Iterate over batch size\n",
    "    for i in range(scores.shape[1]):\n",
    "\n",
    "\n",
    "        # NOTE: Assuming order best model to worst model\n",
    "        # Iterate over models\n",
    "        pred_target = 0\n",
    "        for j in range(scores.shape[0]):\n",
    "            # Get score for specific model\n",
    "            pred_score = scores[j, i]\n",
    "            # TODO: Pick a threshold\n",
    "            threshold = 0.98\n",
    "            if pred_score[0][1] > threshold:\n",
    "                pred_target = 1\n",
    "                break\n",
    "        \n",
    "        # If not relation then do majority vote\n",
    "        if pred_target == 0:\n",
    "            j_scores = np.array([scores[j, i] for j in range(scores.shape[0])])\n",
    "            pred_target = maj_vote_cascade(j_scores)\n",
    "        pred_targets.append(pred_target)\n",
    "\n",
    "    return np.array(pred_targets)\n",
    "\n",
    "\n",
    "# TODO: Cascade classifiers\n",
    "# TODO: Cofidence threshold for the networks if above classify it as it,\n",
    "# otherwise use all three majority vote\n",
    "# TODO: weighted classifiers\n",
    "def get_ensemble(ensemble_name: str):\n",
    "    # TODO: Add more ensemble methods\n",
    "    if ensemble_name == \"high_score\":\n",
    "        return high_score\n",
    "    elif ensemble_name == 'majority_vote':\n",
    "        return majority_vote\n",
    "    elif ensemble_name == 'cascade':\n",
    "        return cascade_classifier\n",
    "    elif ensemble_name == 'thresh_majority_vote':\n",
    "        return threshold_majority_vote\n",
    "    else:\n",
    "        raise RuntimeError('Unkown ensemble name {}'.format(ensemble_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataset auc = {'auc': 0.7435549525101764, 'fmd_auc': 0.7441860465116279, 'fms_auc': 0.7430025445292621}\n"
     ]
    }
   ],
   "source": [
    "# Get the ensemble method function and run it to get predicted targets\n",
    "ensemble_name = 'thresh_majority_vote'\n",
    "ensemble_method = get_ensemble(ensemble_name)\n",
    "pred_targets = ensemble_method(scores)\n",
    "\n",
    "#  Evaluate model\n",
    "auc = eval(pred_targets, test_datasets[0])\n",
    "print('val dataset auc = {}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_res = os.path.join(RESULTS_PATH, \"half_layer_dec_vgg_2020-03-21\")\n",
    "sphere = os.path.join(RESULTS_PATH, \"half_layer_dec_sphere_2020-03-21\")\n",
    "arc = os.path.join(RESULTS_PATH, \"half_layer_dec_arc_2020-03-21\")\n",
    "vgg2 = os.path.join(RESULTS_PATH, \"half_layer_dec_vgg2_2020-03-21\")\n",
    "model_res = [vgg_res, sphere, arc, vgg2]\n",
    "pickles = []\n",
    "for result in model_res:\n",
    "    with open(result, 'rb') as f:\n",
    "        pickles.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.704406779661017,\n",
      " 'fmd_auc': 0.6774425287356322,\n",
      " 'fms_auc': 0.7284980744544287}\n",
      "{'auc': 0.6349152542372881,\n",
      " 'fmd_auc': 0.630028735632184,\n",
      " 'fms_auc': 0.6392811296534018}\n",
      "{'auc': 0.7050847457627119,\n",
      " 'fmd_auc': 0.6673850574712643,\n",
      " 'fms_auc': 0.7387676508344031}\n",
      "{'auc': 0.7410169491525423,\n",
      " 'fmd_auc': 0.7198275862068966,\n",
      " 'fms_auc': 0.7599486521181001}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pickles)):\n",
    "    pprint(pickles[i]['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp-proj",
   "language": "python",
   "name": "mlp-proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
