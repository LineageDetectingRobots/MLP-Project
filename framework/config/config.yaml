# Treat this similarly to json
DEFAULT:
  use_cuda: true
  save_results: true
  experiment_name: default_experiment
  data_settings:
    # Network name choices: 'arc_face', 'vgg_face', 'sphere_face', 'vgg_face2'
    network_name: "arc_face"
  training_settings:
    batch_size: 64
    epochs: 20
    early_stopping: false
    # How many epochs of not increased validation acc. before we stop
    es_epochs: 1
  model_settings:
    model_name: "MLP"
    output_size: 2
    layer_config: [1024, 512, 256, 128, 64, 32, 16, 8, 4]
    use_batch_norm: false
    activation_func: relu
    use_bias: true
    dropout_val: 0.0
    optimiser_type: "adam"
    learning_rate: 0.001

FOUR_DEC:
  experiment_name: four_layer_dec_four
  model_settings:
    layer_config: [1024, 256, 64, 16]

TWO_DEC:
  experiment_name: half_layer_dec
  training_settings:
    epochs: 100
    early_stopping: true
    es_epochs: 5
  model_settings:
    layer_config: [1024, 256]
    activation_func: prelu

DROP_TWO_DEC:
  experiment_name: half_layer_dec
  training_settings:
    epochs: 100
    early_stopping: true
    es_epochs: 5
  model_settings:
    layer_config: [1024, 256]
    dropout_val: 0.5

DROP_EIGHT_SAME:
  experiment_name: drop_eight_same
  training_settings:
    epochs: 100
    early_stopping: true
    es_epochs: 5
  model_settings:
    layer_config: [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]
    dropout_val: 0.5

TRIP_M_LOSS:
  experiment_name: drop_eight_same
  training_settings:
    epochs: 100
    early_stopping: true
    es_epochs: 5
  model_settings:
    layer_config: [1024, 1024, 256, 256, 16, 16, 4, 4]
    activation_func: prelu
    loss_func: TripletMarginLoss


Jack:
  training_settings:
    batch_size: 64
    epochs: 20
    early_stopping: true

Ishan:
  experiment_name: sq_root_decrease
  data_settings:
    # Network name choices: 'arc_face', 'vgg_face', 'sphere_face', 'vgg_face2'
    network_name: "vgg_face2"
  training_settings:
    batch_size: 32
    epochs: 100
    early_stopping: true
    es_epochs: 5
  model_settings:
    layer_config: [1024, 1024, 256, 256, 16, 16, 4, 4]
    activation_func: prelu
    loss_func: nll
